{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:34:23.379481: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-20 15:34:23.379528: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-20 15:34:23.410980: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-20 15:34:25.533270: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-20 15:34:25.533375: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-20 15:34:25.533388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "from umlaut_lte import write_result, get_data_csv, get_data_parquet, plot_loss\n",
    "\n",
    "# folder = 'projects/fl_crowd/'\n",
    "folder = ''\n",
    "\n",
    "result_file = folder + 'results.txt'\n",
    "input_data_file = folder + 'data/df_fl_excerpt.parquet'\n",
    "#input_data_file = folder + 'data/lte.csv'\n",
    "dnn_loss_fig_path = folder + 'dnn-loss-fig.png'\n",
    "logdir = folder + 'tf-logs'  # für tensorboard logs\n",
    "\n",
    "# to be evaluated (depends on data availability and time frame):\n",
    "THRESHOLD_MIN_DAYS_PER_USER = 0\n",
    "THRESHOLD_MIN_DATPOINTS_PER_DAY = 10\n",
    "TEST_SPLIT_SIZE =0.2\n",
    "VALIDATION_SPLIT_SIZE=0.2\n",
    "DNN_BATCH_SIZE = 2\n",
    "\n",
    "# Federated Learning\n",
    "FEDERATED_TRAINING_ROUNDS = 50\n",
    "FEDERATED_LR_CLIENTS = 0.8\n",
    "FEDERATED_LR_SERVER = 3\n",
    "BUFFER_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_keras_model(input_size):\n",
    "    \"\"\"Creates a DNN keras model.\n",
    "\n",
    "    :param input_size: The size of the input of the model\n",
    "    :type input_size: int\n",
    "    :return: The not yet compiled model\n",
    "    :rtype: keras.Model\n",
    "    \"\"\"\n",
    "    # The original model, the actual model below is created by Umlaut\n",
    "    # model = keras.models.Sequential([\n",
    "    #     keras.layers.InputLayer(input_shape=(input_size)),\n",
    "    #     keras.layers.Dense(40, activation='relu', kernel_initializer='zeros'),\n",
    "    #     keras.layers.Dense(10, activation='relu', kernel_initializer='zeros'),\n",
    "    #     keras.layers.Dense(1, kernel_initializer='zeros'),\n",
    "    # ])\n",
    "\n",
    "    print(f'####### input-size {input_size}')\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(input_size)),\n",
    "        keras.layers.Dense(1),\n",
    "    ])\n",
    "    # print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_tf_dnn(X_train, X_test, y_train, y_test, results=result_file,\n",
    "               validation_split_size=VALIDATION_SPLIT_SIZE, batch_size=DNN_BATCH_SIZE):\n",
    "    \"\"\"Creates and trains a model on the given data and writes the results to the given file.\n",
    "\n",
    "    :param X_train: The data to train the model with\n",
    "    :type X_train: pandas.DataFrame\n",
    "    :param X_test: The data to test the model with\n",
    "    :type X_test: pandas.DataFrame\n",
    "    :param y_train: The labels of the training data\n",
    "    :type y_train: pandas.DataFrame\n",
    "    :param y_test: The labels of the data to test the model\n",
    "    :type y_test: pandas.DataFrame\n",
    "    :param results: Path to file where to save the results\n",
    "    :type results: str, optional\n",
    "    :param validation_split_size: The relative part of the data to be used for validation\n",
    "    :type validation_split_size: float, optional\n",
    "    :param batch_size: The size for each bach\n",
    "    :type batch_size: int, optional\n",
    "    \"\"\"\n",
    "    model = create_keras_model(X_train.shape[1])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mse\",\n",
    "        metrics=['mean_absolute_error', RSquare()],\n",
    "    )\n",
    "\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_split=validation_split_size,\n",
    "                        epochs=200,\n",
    "                        verbose=2)\n",
    "\n",
    "    plot_loss(history)\n",
    "    scores = model.evaluate(X_test, y_test)\n",
    "    write_result('Testing tf DNN (mae_loss, mse, r_square):', result_file=results)\n",
    "    write_result(str(scores), result_file=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fed_model_fn():\n",
    "    \"\"\"A function for TFF to create a model during federated learning and return it is the correct type.\n",
    "\n",
    "    :return: The LSTM model to be used as federated models\n",
    "    :rtype: tff.learning.model\n",
    "    \"\"\"\n",
    "    # We _must_ create a new model here, and _not_ capture it from an external\n",
    "    # scope. TFF will call this within different graph contexts.\n",
    "    keras_model = create_keras_model(X_train.shape[1])\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=federated_training_data[0].element_spec,\n",
    "        loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_scikit_lin_reg(X_train, X_test, y_train, y_test, result=\"./results.txt\", validation_split_size=0.2):\n",
    "    \"\"\"Creates and trains a model on the given data and writes the results to the given file\n",
    "\n",
    "    :param X_train: The data to train the model with\n",
    "    :type X_train: pandas.DataFrame\n",
    "    :param X_test: The data to test the model with\n",
    "    :type X_test: pandas.DataFrame\n",
    "    :param y_train: The labels of the training data\n",
    "    :type y_train: pandas.DataFrame\n",
    "    :param y_test: The labels of the data to test the model\n",
    "    :type y_test: pandas.DataFrame\n",
    "    :param results: Path to file where to save the results\n",
    "    :type results: str, optional\n",
    "    :param validation_split_size: The relative part of the data to be used for validation\n",
    "    :type validation_split_size: float, optional\n",
    "    :param batch_size: The size for each bach\n",
    "    :type batch_size: int, optional\n",
    "    \"\"\"\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=validation_split_size, random_state=0)\n",
    "    clf_lr = LinearRegression()\n",
    "    scores = cross_val_score(clf_lr, X_train, y_train.values.ravel(), cv=cv)\n",
    "    write_result(\"scikit LINEAR REGRESSION cross validation: %0.5f mean R^2 with a standard deviation of %0.5f\" % (scores.mean(), scores.std()))\n",
    "    clf_lr.fit(X_train, y_train.values.ravel())\n",
    "    test_score = clf_lr.score(X_test, y_test.values.ravel())\n",
    "    write_result(f'scikit LINEAR REGRESSION test score: {test_score}', result_file=result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_dnn_federated(federated_training_data, X_test, y_test, fed_train_rounds=FEDERATED_TRAINING_ROUNDS,\n",
    "                      result_file=result_file, logdir=logdir):\n",
    "    \"\"\"Create and train a federated DNN model on the given data and writes the result to the result file\n",
    "\n",
    "    :param federated_training_data: The data in a federated format to be trained on\n",
    "    :type federated_training_data: typing.List\n",
    "    :param X_test: The data to test the resulting model with\n",
    "    :type X_test: pandas.DataFrame\n",
    "    :param y_test: The labels to check the test data\n",
    "    :type y_test: pandas.DataFrame\n",
    "    :param result: The file to write the results to\n",
    "    :type result: str, optional\n",
    "    :param logdir: The directory to save the logfiles to\n",
    "    :type logdir: str, optional\n",
    "    \"\"\"\n",
    "    logfile = f'{logdir}/{datetime.now()}'  # für tensorboard log-Dateien\n",
    "\n",
    "    # see if TFF works:\n",
    "    # tff.federated_computation(lambda: 'Initialized!')()\n",
    "    iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "        fed_model_fn,\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.Adam(),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01))\n",
    "\n",
    "    write_result(iterative_process.initialize.type_signature.formatted_representation(), result_file=result_file)\n",
    "    print(\"Test0\")\n",
    "    state = iterative_process.initialize()\n",
    "\n",
    "    print(\"Test1\")\n",
    "    summary_writer = tf.summary.create_file_writer(logfile)\n",
    "    with summary_writer.as_default():\n",
    "        for round_num in range(1, fed_train_rounds):\n",
    "            result = iterative_process.next(state, federated_training_data)\n",
    "            state = result.state\n",
    "            metrics = result.metrics\n",
    "            for name, value in metrics['client_work']['train'].items():\n",
    "                tf.summary.scalar(name, value, step=round_num)\n",
    "\n",
    "    write_result(f'FINISHED federated training, logfile: {logfile}', result_file=result_file)\n",
    "\n",
    "    # Test resulting model\n",
    "    model = create_keras_model(X_test.shape[1])\n",
    "    model_weights = iterative_process.get_model_weights(state)\n",
    "    model_weights.assign_weights_to(model)\n",
    "    model.compile(\n",
    "        loss=tf.losses.mse,\n",
    "        optimizer='adam',\n",
    "        metrics=['mean_absolute_error', RSquare()]\n",
    "    )\n",
    "    scores = model.evaluate(X_test, y_test)\n",
    "    write_result('Testing tf FEDERATED DNN (mae_loss, mse, r_square):', result_file=result_file)\n",
    "    write_result(str(scores), result_file=result_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Calendar_week', 'Weekday', 'Month', 'Weekend', 'Date',\n",
       "       'Count_data_points', 'Velocity_mean', 'Rsrp_mean', 'Rsrq_mean',\n",
       "       'Rssnr_mean', 'Rssi_mean', 'Rsrp_variance', 'Rsrq_variance',\n",
       "       'Rssnr_variance', 'Rssi_variance', 'Brand', 'Wifi_data_points_share',\n",
       "       'Radius_activity_user', 'Radius_activity_state'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(input_data_file)\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "running with threshold 0,data point threshold 10,and test split 0.2...\n",
      "target variable: Radius_activity_user\n",
      "Ignoring velocity? True\n",
      "before filtering data: 50 rows in data set.\n",
      "Index(['Id', 'Calendar_week', 'Weekday', 'Month', 'Weekend', 'Date',\n",
      "       'Count_data_points', 'Velocity_mean', 'Rsrp_mean', 'Rsrq_mean',\n",
      "       'Rssnr_mean', 'Rssi_mean', 'Rsrp_variance', 'Rsrq_variance',\n",
      "       'Rssnr_variance', 'Rssi_variance', 'Brand', 'Wifi_data_points_share',\n",
      "       'Radius_activity_user', 'Radius_activity_state'],\n",
      "      dtype='object')\n",
      "after filtering data: 36 rows in data set.\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "write_result('Loading data...')\n",
    "X_train, X_test, y_train, y_test = get_data_parquet(input_data_file=input_data_file,\n",
    "                                                    threshold_min_days_per_user=THRESHOLD_MIN_DAYS_PER_USER,\n",
    "                                                    test_split_size=TEST_SPLIT_SIZE,\n",
    "                                                    threshold_min_datpoints_per_day=THRESHOLD_MIN_DATPOINTS_PER_DAY)\n",
    "X_train.Weekend = X_train.Weekend.astype(\"float\")\n",
    "X_test.Weekend = X_test.Weekend.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:34:45.549850: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-20 15:34:45.549881: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-20 15:34:45.549902: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (svsram): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "# This needs to be filled with one dataset per client\n",
    "# e.g. a list of tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train), tf.convert_to_tensor(y_train)))\n",
    "# important: these data sets need to be batched, e.g.\n",
    "# ds.shuffle(buffer_size=BUFFER_SIZE, seed=1).batch(DNN_BATCH_SIZE)\n",
    "federated_training_data: List[tf.data.Dataset] = []\n",
    "for brand in X_train.Brand.unique():\n",
    "    df_X = X_train[X_train.Brand == brand].drop(columns=[\"Brand\"])\n",
    "    df_y = y_train[X_train.Brand == brand]\n",
    "    df = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(df_X), tf.convert_to_tensor(df_y)))\n",
    "    federated_training_data.append(df.shuffle(buffer_size=BUFFER_SIZE, seed=1).batch(DNN_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.drop(columns=[\"Brand\"], inplace=True)\n",
    "X_test.drop(columns=[\"Brand\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 14)\n",
      "(2, 14)\n",
      "(2, 14)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfa\n",
    "for dff in federated_training_data:\n",
    "    dffa = tfa.as_numpy(dff.take(1))\n",
    "    i = 0\n",
    "    for el in dffa:\n",
    "        print(el[0].shape)\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FEDERATED DNN...\n",
      "####### input-size 14\n",
      "####### input-size 14\n",
      "####### input-size 14\n",
      "( -> <\n",
      "  global_model_weights=<\n",
      "    trainable=<\n",
      "      float32[14,1],\n",
      "      float32[1]\n",
      "    >,\n",
      "    non_trainable=<>\n",
      "  >,\n",
      "  distributor=<>,\n",
      "  client_work=<>,\n",
      "  aggregator=<\n",
      "    value_sum_process=<>,\n",
      "    weight_sum_process=<>\n",
      "  >,\n",
      "  finalizer=<\n",
      "    int64,\n",
      "    float32[14,1],\n",
      "    float32[1],\n",
      "    float32[14,1],\n",
      "    float32[1]\n",
      "  >\n",
      ">@SERVER)\n",
      "Test0\n",
      "Test1\n",
      "FINISHED federated training, logfile: tf-logs/2023-01-20 15:37:07.787194\n",
      "####### input-size 14\n",
      "1/1 [==============================] - 1s 931ms/step - loss: 223910080.0000 - mean_absolute_error: 9387.7734 - r_square: -0.6493\n",
      "Testing tf FEDERATED DNN (mae_loss, mse, r_square):\n",
      "[223910080.0, 9387.7734375, -0.64928138256073]\n",
      "DONE (FEDERATED DNN)\n",
      "DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "write_result('Starting FEDERATED DNN...')\n",
    "run_dnn_federated(federated_training_data, X_test, y_test)\n",
    "write_result('DONE (FEDERATED DNN)')\n",
    "\n",
    "write_result('DONE\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
